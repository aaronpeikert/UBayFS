---
title: "A quick tour through UBayFS"
author: "Anna Jenul, Stefan Schrunner"
date: "`r Sys.Date()`"
output: 
  prettydoc::html_pretty:
    theme: architect
    toc: TRUE
    hightlight: github
bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{UBayFS general}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style>
  body {
  text-align: justify;
  padding: 1em}
</style>


```{r, include = FALSE}
library(knitr)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(fig.height = 5)
knitr::opts_chunk$set(fig.width = 8)
knitr::opts_chunk$set(fig.align = 'center')
```


## Introduction
The UBayFS package implements the framework proposed in [@jenul:UBayFS], together with an interactive Shiny dashbord, which makes UBayFS applicable to R-users with different levels of expertise. UBayFS is an ensemble feature selection technique, embedded in a Bayesian statistical framework. The method combines data and user knowledge, where the first is extracted via data-driven ensemble feature selection. The user can control the feature selection by assigning prior weights to features and penalizing specific feature combinations. In particular, the user can define a maximal number of selected features and must-link constraints (features must be selected together) or cannot-link constraints (features must not be selected together). Using relaxed constraints, a parameter $\rho$ regulates the penalty shape. Hence, violation of constraints can be valid but leads to a lower target value of the feature set that is derived from the violated constraints.

In this vignette we use the Breast Cancer Wisconsin dataset [@wolberg:wisconsin] for demonstration. Specifically, the dataset consists of 569 samples and 30 features, and can be downloaded as demo function using the function ``data(bcw)``. The dataset describes a classification problem, where the aim is to distinguish between malignant and benign cancer based on image data. Features are derived from 10 image characteristics, where each characteristic is represented by 3 features (summary statistics) in the dataset. For instance, the characteristic *radius* is represented by features *radius mean*, *radius standard deviation* and *radius worst*.

### Requirements and dependencies
- R (>= 3.5.0)
- GA
- matrixStats
- shiny
- mRMRe
- Rdimtools
- caret
- DirichletReg
- glmnet
- ggplot2
- ggpubr
- utils
- hyper2
- rpart
- GSelection
- knitr


In addition, some functionality of the package (in particular, the interactive Shiny interface) requires the following depedencies:

- shinyWidgets
- shinyalert
- DT
- RColorBrewer
- tcltk
- shinyjs
- shinythemes
- shinyBS
- testthat (>= 3.0.0)
- rmarkdown
- prettydoc
- plyr

UBayFS is implemented via a core S3-class 'UBaymodel', along with help functions. An overview of the 'UBaymodel' class and its main generic functions, is shown in the following diagram:

```{r, out.width="70%", out.height="70%", fig.align="center", echo = FALSE}
include_graphics("UBay_UML.jpg")
``` 

The help function ``buildConstraints(...)`` provides an easy way to define side constraints for the model. Further, ``runInteractive()`` enters the Shiny dashboard, given that the required depedencies are available (see above).

Like other R packages, UBayFS is loaded using the ``library(UBayFS)`` command. The sample dataset is accessed via ``data(bcw)``.

```{r, include = TRUE, cache = TRUE}
library(UBayFS)
data(bcw)
```

## Background
This section summarizes the core parts of UBayFS, where a central part is the Bayes Theorem for two random variables $\theta$ and $y$:
$$p(\theta|y)\propto p(y|\theta)\cdot p(\theta).$$

### Ensemble feature selection as likelihood
The first step in UBayFS is to build $M$ ensembles of elementary feature selectors  --- each elementary feature selector $1,\dots,M$ selects features, denoted by a binary membership vector $\delta^{(m)} \in \{0,1\}^N$, based on a randomly selected training dataset, where $N$ denotes the total number of features in the dataset. In the binary membership vector $\delta^{(m)}$, a component $\delta_i^{(m)}=1$ indicates that feature $i\in\{1,\dots,N\}$ is selected, and $\delta_i^{(m)}=0$ otherwise. Statistically, we interpret the result from each elementary feature selector as realization from a multinomial distribution with parameters $\theta$ and $l$, where $\theta\in[0,1]^N$ defines the success probabilities of sampling each feature in an individual feature selection and $l$ corresponds to the number of feature selected in $\delta^{(m)}$. Therefore, the joint probability density of the observed data --- the likelihood function --- has the form 
$$ p(y|\theta) = \prod\limits_{m=1}^{M} f_{\text{mult}}(\delta^{(m)};\theta,l),$$
where $f_{\text{mult}}$ is the probability density function of the multinomial distribution.

### Expert knowledge as prior
UBayFS includes two types of expert knowledge: prior feature weights and feature set constraints. 

#### Prior feature weights

To introduce expert knowledge about feature importance, the user may define a vector $\alpha = (\alpha_1,\dots,\alpha_N)$, $\alpha_i>0$ for all $i=1,\dots,N$, assigning a weight to each feature. High weights indicate that a feature is important. By default, if all features are equally important or no prior weighting is used, $\alpha$ is set to the 1-vector of length $N$. With the weighting in place, we assume the a-priori feature importance parameter $\theta$ follows a Dirichlet distribution [@R:DirichletReg]
$$p(\theta) = f_{\text{Dir}}(\theta;\alpha),$$
where the probability density function of the Dirichlet distribution is given as
$$f_{\text{Dir}}(\theta;\alpha) = \frac{1}{\text{B}(\alpha)} \prod\limits_{n=1}^N \theta_n^{\alpha_n-1},$$
where $\text{B}(.)$ denotes the multivariate Beta function. Generalizations of the Dirichlet distribution [@wong:gdirichlet,@hankin:hyperdirichlet] are implemented in UBayFS as well.

Since the Dirichlet distribution is the conjugate prior of the multinomial distribution, the posterior density is given as
$$p(\theta|y) \propto f_{\text{Dir}}(\theta;\alpha^\circ),$$
with
$$\alpha^\circ = \left( \alpha_1 + \sum\limits_{m=1}^M \delta_1^{(m)}, \dots, \alpha_N + \sum\limits_{m=1}^M \delta_N^{(m)}  \right)$$
representing the posterior parameter vector $\alpha^\circ$.

#### Feature set constraints
In addition to the prior weighting of features, the UBayFS user can also add different types of constraints to the feature selection:

- *max-size constraint*: maximal number of features that shall be selected.
- *must-link constraint*: used if a set of features must be selected at the same time (defined as pairwise constraints, one for each pair of features).
- *cannot-link constraint*: used if a set of features must not be selected at the same time.
- *block-wise constraints*: define constraints between feature blocks (instead of individual features).

Constraints are represented as a linear optimization problem $A\delta-b\leq 0$, where $A\in\mathbb{R}^{K\times N}$ and $b\in\mathbb{R}^K$. $K$ is defined as the total number of constraints. In general, a feature set $\delta$ is admissible only if $\left(a^{(k)}\right)^T\delta - b^{(k)} \leq 0$, according to the inadmissibility function (penalty term)
$$ \kappa_k(\delta) = \left\{\begin{array}{l l}
    0 & \text{if}~ \left(a^{(k)}\right)^T\delta - b^{(k)} \leq 0 \\
    1 & \text{otherwise},\end{array}\right.$$

In UBayFS, $\kappa_k(\delta)$ is substituted by a relaxed inadmissibility function $\kappa_{k,\rho}(\delta)$, given as

\begin{align}
\kappa_{k,\rho}(\delta) = \left\{
    \begin{array}{l l}
    0 & \text{if}~\left(a^{(k)}\right)^T\delta\leq b^{(k)}\\
    1 & \text{if}~ \left(a^{(k)}\right)^T\delta> b^{(k)} \land \rho =\infty\\
    \frac{1-\xi_{k,\rho}}{1 + \xi_{k,\rho}} & \text{otherwise},
    \end{array}
    \right.
\end{align}
    
where $\rho\in\mathbb{R}^+ \cup \{\infty\}$ denotes a relaxation parameter and
$\xi_{k,\rho} = \exp\left(-\rho \left(\left( a^{(k)}\right)^T\delta - b^{(k)}\right)\right)$ defines the exponential term of a logistic function. To handle $K$ different constraints for one feature selection problem, the joint inadmissibility function is given as
$$ \kappa(\delta)
    = 1 - \prod\limits_{k=1}^{K} \left(1 -\kappa_{k,\rho}(\delta)\right)$$
which originates from the idea that $\kappa = 1$ (maximum penalization) if at least one $\kappa_{k,\rho}=1$, while $\kappa=0$ (no penalization) if all $\kappa_{k,\rho}=0$. 

The target function to select an optimal feature set $\delta$ is given by an expected utility given the posterior parameter $\delta | y$, as well as the inadmissibility function $\kappa(.)$.

$$\mathbb{E}_{\theta|y}[U(\delta, \theta(y))] = \delta^T \mathbb{E}_{\theta|y}[\theta(y)]-\lambda\kappa(\delta)\longrightarrow \underset{\delta\in\{0,1\}^N}{\text{max}}
$$
    
Since an exact optimization is impossible due to the non-linear function $\kappa$, we use a genetic algorithm to find an appropriate feature set. In detail, the genetic algorithm is initialized via a Greedy algorithm and computes combinations of the given feature sets with regard to a fitness function in each iteration.

## Ensemble Training
The function ``build.UBaymodel()`` initializes the UBayFS model and performs ensemble feature selection. The training dataset and target are initialized with the arguments ``data`` and ``target``. Although the UBayFS concept permits unsupervised, multiclass or regression setups, the current implementation supports binary target variables only. The input variable ``M`` defines the number of ensembles of each feature selector in ``method`` shall be performed. Each ensemble model is computed on ``tt_split`` percent randomly selected samples of the whole sample space. Currently, the ``method`` parameter can be set to ``mRMR`` (minimum redundancy maximum relevance), see [@ding:mrmr], and ``laplace`` (Laplacian score), see [@he:laplacian]. The argument ``prior_model`` specifies whether the standard Dirichlet distribution, or a generalized variant should be used as prior model.
Furthermore, the number of features selected in each ensemble can be controled with the parameter ``nr_features``.

For the standard UBayFS initialization, all prior feature weights are set to 1 and no feature constraints are included, yet. The ``summary()`` function provides an overview  of the dataset, the prior weights and the likelihood --- ensemble counts indicate how often a feature was selected over the ensemble feature selections. 
```{r, include = TRUE}
model = build.UBaymodel(data = bcw$data,
                        target = bcw$labels,
                        M = 100, 
                        tt_split = 0.75,
                        nr_features = 10,
                        method = "mRMR",
                        prior_model ="dirichlet",
                        weights = 0.01,
                        lambda = 1,
                        constraints = buildConstraints(constraint_types = c("max_size"), 
                                                       constraint_vars = list(3), 
                                                       num_elements = dim(bcw$data)[2], 
                                                       rho = 1),
                        block_constraints = NULL,
                        optim_method = "GA",
                        popsize = 100,
                        maxiter = 100,
                        shiny = FALSE
                        )
summary(model)
```
## User knowledge
With the function ``setWeights()`` the user can change the feature weights from the standard initialization to desired values. In our example dataset we assign features representing common image characteristics the same weight.  
```{r, include=TRUE}
weights = rep(c(10,15,20,16,15,10,12,17,21,14), 3)
print(weights)

model = setWeights(model = model, 
                   weights = weights)
```

Feature constraints can defined with the function ``buildConstraints()``. The input ``constraint_types`` consists of a vector, where all constraint types are defined. Then, with ``constraint_vars``, the user specifies details about the constraint: for max-size, the number of features to select is provided, while for must-link and cannot-link, the set of feature indices to be linked must be provided. Each list entry corresponds to one constraint in ``constraint_types``. In addition, ``num_features`` denotes the total number of features in the dataset and ``rho`` corresponds to the relaxation parameter of the admissibility function.  

As ``print(constraints)`` shows, the matrix ``A`` has 10 rows for 4 constraints. While *max-size* and *cannot-link* can be expressed in one equation each, *must-link* is a pairwise constraint. In specific, the *must-link* constraint between $n$ features produces $\frac{n!}{(n-2)!}$ elementary constraints. Hence, 6 equations represent the *must-link* constraint. The function ``setConstraints()`` integrates the constraints into the UBay model. 
```{r, include=TRUE}
constraints = buildConstraints(constraint_types = c("max_size", 
                                                    "must_link", 
                                                    rep("cannot_link", 2)),
                               constraint_vars = list(10, # max_size
                                                      c(1,11,21), # must_link
                                                      c(1,10), # cannot_link
                                                      c(20,23,24)), #cannot_link
                               num_elements = ncol(model$data),
                                rho = c(Inf, # max_size
                                        0.1, # must_link
                                        1, # cannot_link
                                        1)) # cannot_link
print(constraints)

model = setConstraints(model = model, constraints = constraints)
model = setBlockConstraints(model = model, constraints = NULL)
```

## Optimization and evaluation
A genetic algorithm, described by [@givens:compstat] and implemented in [@R:GA], searches for the optimal feature set in the UBayFS framework. With ``setOptim()`` we initialize the genetic algorithm. ``popsize`` indicates the number of candidate feature set created in each iteration and ``maxiter`` is number of iterations. 
  - train function with GA
  - print/summary/plot

```{r, include=TRUE}
model = setOptim(model = model,
                 popsize = 100, 
                 maxiter = 200)

```

At this point, we have initialized prior weights, constraints and the optimization procedure --- we can now train the UBayFS model with the generic function ``train``. The summary function provides an overview on all components, UBay exists of. The ``plot()`` function shows the prior feature information as bar charts, with the selected features marked with red borders. In addition, the constraints and the regularization parameter $\rho$ are presented. 
```{r, include=TRUE, fig.width=7, fig.height=6}
model = train(x = model)
summary(model)
plot(model)
```
The plot shows the selected features (red framed) and their selection-distribution between ensemble feature selection and prior weights. The constraints are shown in the top where a connecting line is drawn between features of one constraint.
The final feature set and its admissibility can be evaluated with ``evaluateFS()`` and ``admissibility()``:
```{r, include=TRUE}
evaluateMultiple(state = model$output$feature_set, model = model)
#admissibility(state = unlist(model$output$feature_set), constraints = constraints)
```
## Shiny dashboard
``UBayFS`` provides an interactive ``R Shiny`` dashboard as GUI. With its intuitive user interface, the user can load data, set likelihood parameters and even control the admissibility regularization strength of each constraint. Histograms and further plots help to get an overview on the users settings. The interactive dashboard offers ``save`` and ``load`` buttons, in order to save or load UBayFS models as RData files.
```{r,eval=FALSE}
runInteractive()
```

```{r, out.width="100%", echo = FALSE}
include_graphics("UBay_Shiny_Screenshot.png")
``` 

## Conclusion
Although the current version of UBayFS is limited to ``mRMR`` and ``Laplacian score``, it will be extended with additional feature selectors that provide a scoring of the features. With the methodology in place, UBayFS is applicable to a large range of feature selection problems with multiple sources of information. The likelihood parameters, steering the number of elementary models, mainly affect the stability and runtime of the result --- the latter linearly increases with the number of models. Especially the Shiny dashboard delivers insight into the single UBayFS steps. Nevertheless, the dashboard is only applicable for smaller datasets, while larger datasets should be computed in the console. 


## References



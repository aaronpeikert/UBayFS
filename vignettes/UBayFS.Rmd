---
title: "A quick tour through UBayFS"
author: "Anna Jenul, Stefan Schrunner, Oliver Tomic,  JÃ¼rgen Pilz"
date: "Feb 2021"
output: 
  prettydoc::html_pretty:
    theme: architect
    toc: TRUE
    hightlight: github
vignette: >
  %\VignetteIndexEntry{UBayFS}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style>
  body {
  text-align: justify;
  padding: 1em}
</style>


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(knitr)
```


## Introduction
Feature selection plays a key role in data prepocessing, in order to reduce complexity, prevent overfitting of machine learning models and to reduce runtime. Furthermore, feature selection delivers insights into the systematic variation in the data, which is crucial in domains that rely on model interpretability, such as life sciences. UBayFS is an ensemble feature selection technique, embedden in a Bayesian statistical framework. With our approach we combine data and user knowledge, where the first is extracted from data driven ensemble feature selection techniques. The user can control the feature selection by assigning prior weights to features and penalizing specific feature combinations. Also, the user can define a maximal number of selected features and mustlink-constraints (features must be selected together) or cannotlink-constraints (features must not be selected together). A parameter $\rho$ regulates the penalty shape --- violation of constraints can be valid but leads to a lower probability of the feature set that is derived from the violated constraints.

The UBayFS package implements the framework proposed in linktopaper, together with an interactive Shiny dashbord, which makes UBayFS applicable to R-users as well as non R-users. In this documentation we use the Wisconsin breast cancer dataset (reference) for demonstration. Specifically, the dataset consists of 569 samples and 30 features, and can be downloaded from ..link.. with the UBay function ``loadWisconsin()``. The features are derived from 10 characteristics, where each characteristic is represented by 3 summary statistics in the dataset. Thus, the characteristic *radius* results in features *radius mean*, *radius standard deviation* and *radius worst*.

% requirements and dependencies

```{r, include = TRUE, cache = TRUE}
library(UBayFS)
demo = loadWisconsin()
```

UBayFS - S3 classes etc? - wesentliche Funktionen

```{r, out.width="50%", out.height="50%", fig.align="center", echo = FALSE}
include_graphics("UBay_UML.png")
``` 


## Background
Bayesian approach - likelihood and prior

### Ensemble feature selection as likelihood
The first step in UBayFS is to build an $M$-dimensional ensemble of elementary feature selectors  --- each elementary approach $\zeta_1\dots \zeta_M$ selects features from a randomly drawn subset of the sample-space of the train dataset and thus, delivers a proposal for an optimal feature set. The number of feature selected with $\zeta_m, m\in 1\dots M$ deonted as $S_m \subset \{1\dots N\}$, where $N$ is assigned the total number of features in the train dataset. In addition, $\delta^{S_m}$ is the binary membership vector for feature selection $m$, where 1 indicates that feature $i\in\{1,\dots,N\}$ is selected, and 0 if not. Statistically, we interpret the result from each elementary feature selector as realization from a multinomial distribution with parameters $\theta$ and $l$, where $\theta$ defines the success probabilities of sampling each feature in an individual feature selection and $l$ corresponds to the number of feature selected in that selection. Therefore, the distribution of the observed data is of the form 
$$ p(\Delta|\theta) = \prod\limits_{m=1}^{M} f_{\text{mult}}(\delta^{S_m};\theta,l).$$

### Expert knowledge as prior
UBayFS includes to types of expert knowlegde, prior feature weighting and prior feature constraints. 

#### Prior feature weights

To add feature importances as prior information to UBayFS, the user can define a vector $\alpha = (\alpha_1,\dots,\alpha_N)$, assigning each feature a weight. The higher the weight, the more important the feature. If all features are equally important or no prior weighting is used, $alpha$ is the $N$-dimensional 1-vector. With the weighting in place, we assume that the a priori feature importances $\theta$ follow a Dirichlet distribution 
$$p(\theta) = f_{\text{Dir}}(\theta;\alpha),$$
where the Dirichlet distribution is defined as 
$$f_{\text{Dir}}(\theta;\alpha) = \text{Be}(\alpha)^{-1} \prod\limits_{n=1}^N \theta_n^{\alpha_n-1},$$
where $\text{Be}(.)$ denotes the multivariate Beta function. Furthermore, the Dirichlet distribution is the conjugate prior of the multinomial distribution, what makes it possible to explicitly state the posterior density distribution of $\theta$ given the data as
$$p(\theta|\Delta) \propto f_{\text{Dir}}(\theta;\alpha^0),$$
with 
$$\alpha^0 = \left( \alpha_1 + \sum\limits_{m=1}^M \delta_1^{S_m}, \dots, \alpha_N + \sum\limits_{m=1}^M \delta_N^{S_m}  \right)$$


#### Prior feature constraints
In addition to the prior weighting of features, the UBayFS user can also add different types of constraints to the feature selection:

- *max size constraint*: maximal number of features that shall be selected.
- *must link constraint*: used if a set of features must be selected together. (defined as binary constraints for each feature pair)
- *cannot link constraint*: used if a set of featuers must not be selected together.

The constraints are represented as a linear optimization problem $A\cdot d_\theta-b\leq 0$, where $A\in\mathbb{R}^{K\times N}$ and $b\in\mathbb{R}^K$. $K$ is defined as the total number of constraints. In generall, a feature importance vector $\theta$ would be  admissible only if $\langle a^{(k)},\delta_\theta \rangle - b^{(k)} \leq 0.$, due to the admsiisibility function
$$ ad_k(\theta) = \left\{\begin{array}{ll}
    1 & \text{if}~\langle a^{(k)}, \delta_\theta \rangle - b^{(k)} \leq 0 \\
    0 & \text{else},\end{array}\right.$$

Though, in UBay we introduce a relaxed admissibility function, which makes it possible to accept also points that lie outside the admissibility space, but giving a higher penalty when chosen. To control the penalty strength, a relaxation paramter $\rho\in\mathbb{R}^+ \cup {\infty}$ is used:

\begin{align}
ad_{k,\rho}(\theta) = \left\{
    \begin{array}{c c}
    \frac{\xi_{k,\rho}}{1 + \xi_{k,\rho}} & \rho \in\mathbb{R}^{+}\\
    ad_{k}(\theta) & \rho = \infty.
    \end{array}
    \right.
\end{align}
    
    
$\xi_{k,\rho} = \exp\left(-\rho (\langle a^{(k)}, \delta_\theta \rangle - b^{(k)} - 0.5)\right)$ defines the exponential term of a logistic function. To handle $K$ different constraints for one feature selection problem, we define the joint admissibility function

$$ \kappa(\theta) = \prod\limits_{k=1}^K ad_{\kappa,\rho}(\theta).$$
With this definition, we define the function $f_\kappa(\theta)\propto (1-\kappa(\theta))$, which is proportinal to a probability density function. To comine prior weighting and prior constraints, we define the composite prior density as $$p(\theta) \propto f_\kappa(\theta)\cdot f_{\text{Dir}}(\theta;\alpha).$$


The posterior density of $\theta$ can now be defined as

\begin{align}
    p(\theta|\Delta)
    &\propto \left(\prod\limits_{m = 1}^{M} f_{\text{mult}}(\delta^{S_m};\theta,l) \right)\cdot \left(f_{\kappa}(\theta) \cdot f_{\text{Dir}}(\theta;\alpha)\right) \nonumber\\
    &\propto f_{\kappa}(\theta) \cdot f_{\text{Dir}}(\theta;\alpha^{\circ}).
\end{align}
    
From a statistical perspective, we try to find the maximum a posteriori value of $p(\theta|\Delta)$. An exact optimization is impossible --- therefore, we use a heuristic genetic algorithm to find an appropriate feature set. In detail, the genetic algorithm starts with one random or predefined feature combination, iterates through the feature set and updates if another combination is more appropriate with regard to a fitness function.

## Ensemble Training
The function ``build.UBaymodel()`` initializes the UBayFS and performs ensemble feature selection. The training dataset and target are initialized with the arguments ``data`` and ``target``. The input variable ``M`` defines the number of unique feature selections of each feature selector in ``method`` shall be performed on different, random subsets of the samples from the train dataset (``tt_split`` percent of the whole sample space). Currentyly ``"mRMR"`` (minimum redundancy maximum relevane) and ``"lscore"`` (Laplace score) are implemented and can be used in ``method``. Furthermore, the number of features selected in each ensemble can be controled with the parameter``nr_features``.

For the standard UBay initialization, all prior feature weights are set to 1 and no prior feature constraints are included, yet. The ``summary()`` function provides an overview  of the dataset, the prior weights and the likelihood --- ensemble counts indicate how often a feature was selected ofter the ensemble feature selections. 
```{r, include = TRUE}
model = build.UBaymodel(data = demo$data,
                        target = demo$labels,
                        M = 100, 
                        tt_split = 0.75,
                        nr_features = 10,
                        method = "mRMR")
summary(model)
```
## User knowledge
With the function ``setWeights()`` the user can change the feature weights from the standard initialization to desired values. In our example dataset we assign each feature originating from one characteristic the same weight.  
```{r, include=TRUE}
weights = rep(c(10,15,20,16,15,10,12,17, 21,14), 3)
print(weights)

model = setWeights(model=model, weights = weights)
```

Prior feature constraints can defined with the function ``buildConstraints()``. The input ``constraint_types`` consists of a vector, where all the constraint types are defined. Then, with ``constraint_vars``, the user advises the program which features are affected for each constraint type. The order of the constrain variables list is sensitive to the order of the constraint types. In addition, ``num_features`` denotes the total number of features in the dataset and ``rho`` corresponds to the relaxation parameter of the admissibility function.  

As the ``print(constraints)``shows, the matrix ``A`` has 10 rows for 4 constraints. While *max size* and *cannot link* can be expressed by one equation, *must link* is a pairwise constraint, taking order into account. Therefore, we calculate the number of equations for one *must link* constraint with the formula $$\frac{n!}{(n-2)!},$$ where $n$ is the total number of features in the constraint. With this formula, we get 6 equations for our *must link* constraint. The function ``setConstraints()`` integrates the constraints into the UBay model. 
```{r, include=TRUE}
constraints = buildConstraints(constraint_types = c("max_size", "must_link", rep("cannot_link", 2)),
                               constraint_vars = list(10, # max_size
                                                      c(1,11,21), # must_link
                                                      c(1,10), # cannot_link
                                                      c(20,23,24) #cannot_link
                                                      ),
                               num_features = ncol(model$data),
                                rho = c(Inf, # max_size
                                        0.1, # must_link
                                        1, # cannot_link
                                        1) # cannot_link
)
# )
print(constraints)

model = setConstraints(model = model, constraints = constraints)
```

## Optimization and evaluation
To approximate the maximum a posteriori estimate for the optimal feature set according to the Bayesian framework, we use heuristic search with genetic algorithm. (used alg. from package bla bla). With ``setOptim()`` we initialize the genetic algorithm. ``popsize`` inicates the number of candidate feature set created in each iteration and ``maxiter`` is the iteration size. 
- train function with GA
- print/summary/plot

```{r, include=TRUE}
model = setOptim(model = model, popsize = 100, maxiter = 200)

```

At this point, we are have initialized prior weights, prior constraints and the optimization procedure --- we can now train the UBayFS model with the function ``train``. The summary function provides an overview on all components, UBay exists of. The ``summary$map`` entry reveils the final feature set. The ``plot()`` function shows the prior feature information in form of bar charts, with the final feature set enframed with red color. In addition, the constraints and the regularization parameter $\rho$ are presented. 
```{r, include=TRUE, fig.width=7, fig.height=6}
model = train(x = model)
summary(model)
plot(model)
```


## Shiny dashboard
``UBayFS`` provides an interactive ``R Shiny`` dashboard, making UBayFS applicable to non-R users, as well. With its intuitive guidance, the user can load data, set likelihood parameters and even control the admissibility regularization strength of each constraint. Histograms and further plots help to get an overview on the users settings. The interactive dashboard offers ``save`` and ``load`` buttons, in order to save or load UBay models. 
```{r,eval=FALSE}
runInteractive()
```

```{r, out.width="100%", echo = FALSE}
include_graphics("UBay_Shiny_Screenshot.png")
``` 

## Conclusion
runtime increases linearly with the number of models;
we also can add further feature selection approaches to the ensemble
future topics

## References


